# Import libraries
import pandas as pd
import numpy as np
import dask.dataframe as dd
import lightgbm as lgb
import pickle
import logging


# Function to convert data types to reduce memory
def convert_types(df):
    # Convert data types to reduce memory
    for c in df:
        col_type = str(df[c].dtypes)
        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']

        # Convert objects to category
        if col_type == 'object':
            df[c] = df[c].astype('category')

        # numerics
        elif col_type in numerics:
            c_min = df[c].min()
            c_max = df[c].max()
            if col_type[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[c] = df[c].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[c] = df[c].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[c] = df[c].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[c] = df[c].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[c] = df[c].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[c] = df[c].astype(np.float32)
                else:
                    df[c] = df[c].astype(np.float64)

    return df


# Set up basic configuration for logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')


def load_data(file_path):
    # Function to load and preprocess data
    logging.info(f"Loading data from {file_path}")
    ddf = dd.read_csv(file_path)
    df = ddf.compute()
    df = convert_types(df)
    logging.info(f"Data loaded from {file_path}")
    return df


# Import Data
train = load_data('train_refine.csv')
test = load_data('test_refine.csv')

# Prepare data for training and prediction
target = 'HasDetections'
data_id = 'MachineIdentifier'

# For Train Dataset
train_labels = np.array(train[target].astype(np.int8)).reshape((-1, ))
train_features = train.drop(columns=[target, data_id])

# For Test Dataset - only drop the data_id as the target column might not be present
test_features = test.drop(columns=[data_id])
test_ids = list(test[data_id])


# LightGBM parameters
# Categorical features for LighGBM parameter
categorical_feature = ['OsPlatformSubRelease',
                       #    'Census_MDC2FormFactor_new',
                       'Census_FlightRing',
                       'Census_PrimaryDiskTypeName',
                       'Census_OSSkuName',
                       'Census_OSBranch',
                       'OsVer',
                       'SkuEdition',
                       'Census_OSArchitecture',
                       'Census_OSEdition',
                       'Census_GenuineStateName',
                       'Processor',
                       'SmartScreen',
                       'Census_OSInstallTypeName',
                       'Census_OSWUAutoUpdateOptionsName',
                       'Census_ChassisTypeName',
                       'Census_MDC2FormFactor',
                       'Platform',
                       'Census_DeviceFamily',
                       'Census_ActivationChannel',
                       'Census_PowerPlatformRoleName',
                       'Census_InternalPrimaryDiagonalDisplaySizeInInches',
                       'Census_ProcessorCoreCount',
                       'Census_HasOpticalDiskDrive',
                       'Census_IsAlwaysOnAlwaysConnectedCapable',
                       'Census_IsPenCapable',
                       'Census_IsPortableOperatingSystem',
                       'Census_IsSecureBootEnabled',
                       'Census_IsTouchEnabled',
                       'Census_IsVirtualDevice',
                       'Firewall',
                       'HasTpm',
                       'IsProtected',
                       'IsSxsPassiveMode',
                       'SMode',
                       'Wdft_IsGamer']

best_hyp = {'boosting_type': 'gbdt',
            'class_weight': None,
            'colsample_bytree': 0.6110437067662637,
            'learning_rate': 0.0106,
            'min_child_samples': 295,
            'num_leaves': 160,
            'reg_alpha': 0.6321152748961743,
            'reg_lambda': 0.6313659622714517,
            'subsample_for_bin': 80000,
            'subsample': 0.8202307264855064}

estimators = 5000

# Ensure categorical features are of type 'category' in both datasets
for feature in categorical_feature:
    if feature in train_features.columns:
        train_features[feature] = train_features[feature].astype('category')
    if feature in test_features.columns:
        test_features[feature] = test_features[feature].astype('category')

print("Categorical features used in model:", categorical_feature)


# Train model
model = lgb.LGBMClassifier(n_estimators=estimators, n_jobs=-1,
                           objective='binary', random_state=50, **best_hyp)
model.fit(train_features, train_labels,
          categorical_feature=categorical_feature)

# Predict on Test set
preds = model.predict_proba(test_features)[:, 1]


# Train model
model = lgb.LGBMClassifier(n_estimators=estimators, n_jobs=-1, objective='binary',
                           random_state=50, verbose=100, **best_hyp)
logging.info("Starting model training")
model.fit(train_features, train_labels,
          categorical_feature=categorical_feature)
logging.info("Model training completed")


# Save model
pickle.dump(model, open("Model/model_lgbm.p", "wb"))

# Predict on Test set
preds = model.predict_proba(test_features)[:, 1]

# Create and save result
result = pd.DataFrame({data_id: test_ids, target: preds})
result.to_csv('Result/result_lgbm.csv', index=False)

# Feature Importance
feature_names = list(train_features.columns)
feature_importances = pd.DataFrame(
    {'feature': feature_names, 'importance': model.feature_importances_})
feature_importances = feature_importances.sort_values(
    'importance', ascending=False).reset_index(drop=True)
feature_importances['normalized_importance'] = feature_importances['importance'] / \
    feature_importances['importance'].sum()
feature_importances['cumulative_importance'] = np.cumsum(
    feature_importances['normalized_importance'])
feature_importances.to_csv(
    'Feature_importances/FeatureImportance_lgbm.csv', index=False)

logging.info("Script completed successfully")
